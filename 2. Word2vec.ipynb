{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305b9ff2",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\" width=150px> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>ChatGPT And Friends</h1>\n",
    "<h1>Word2vec</h1>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04da141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import datasets\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c83f9f",
   "metadata": {},
   "source": [
    "We start by printing out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb51e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.21.0\n",
      "\n",
      "Compiler    : Clang 15.0.0 (clang-1500.1.0.2.5)\n",
      "OS          : Darwin\n",
      "Release     : 23.3.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 2305f34fce93e0f71d7791f555117afb877e0f0c\n",
      "\n",
      "matplotlib: 3.8.2\n",
      "json      : 2.0.9\n",
      "re        : 2.2.1\n",
      "tqdm      : 4.66.1\n",
      "networkx  : 3.2.1\n",
      "pandas    : 2.2.0\n",
      "datasets  : 1.16.1\n",
      "watermark : 2.4.3\n",
      "numpy     : 1.26.4\n",
      "scipy     : 1.12.0\n",
      "torch     : 2.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b9c79",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c3afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf25a63",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7844aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/bgoncalves/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/3e953745870454cf8ff15cc48097dbb5ff459596e0a089867c2a29cee63984ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed32bc9a06e04af3bc08e7ae77599471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('tweets_hate_speech_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc3093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function split_tokens at 0x1748a25c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ae406493e94fff92d49a2cb4a5b776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31962 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def split_tokens(row):                             # STEP\n",
    "    row['all_tokens'] = [ss.stem(i) for i in       # 5\n",
    "                     re.split(r\" +\",               # 3\n",
    "                     re.sub(r\"[^a-z@# ]\", \"\",      # 2\n",
    "                            row['tweet'].lower())) # 1\n",
    "                     if (i not in sw) and len(i)]  # 4\n",
    "    return row\n",
    "\n",
    "# Determine vocabulary so we can create mapping\n",
    "dataset = dataset.map(split_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89df6f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab5bccf5c234a399961384e4f2a2d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31962 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = Counter([i for s in dataset['train']['all_tokens'] for i in s])\n",
    "counts = {k:v for k, v in counts.items() if v>10} # Filtering\n",
    "vocab = list(counts.keys())\n",
    "n_v = len(vocab)\n",
    "id2tok = dict(enumerate(vocab))\n",
    "tok2id = {token: id for id, token in id2tok.items()}\n",
    "\n",
    "# Now correct tokens\n",
    "def remove_rare_tokens(row):\n",
    "    row['tokens'] = [t for t in row['all_tokens'] if t in vocab]\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(remove_rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2038a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c535e07e1e64d2d866fdfb0388c789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31962 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def windowizer(row, wsize=3):\n",
    "    \"\"\"\n",
    "    Windowizer function for Word2Vec. Converts sentence to sliding-window\n",
    "    pairs.\n",
    "    \"\"\"\n",
    "    doc = row['tokens']\n",
    "    wsize = 3\n",
    "    out = []\n",
    "    for i, wd in enumerate(doc):\n",
    "        target = tok2id[wd]\n",
    "        window = [i+j for j in\n",
    "                  range(-wsize, wsize+1, 1)\n",
    "                  if (i+j>=0) &\n",
    "                     (i+j<len(doc)) &\n",
    "                     (j!=0)]\n",
    "\n",
    "        out+=[(target, tok2id[doc[w]]) for w in window]\n",
    "    row['moving_window'] = out\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(windowizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76882a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "BATCH_SIZE = 2**14\n",
    "N_LOADER_PROCS = 10\n",
    "\n",
    "dataloader = {}\n",
    "for key in dataset.keys():\n",
    "    dataloader = {key: DataLoader(Word2VecDataset(\n",
    "                                    dataset[key], vocab_size=n_v),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=N_LOADER_PROCS)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f6a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], requires_grad=True)\n",
      "tensor([3.], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "input = 3\n",
    "\n",
    "def one_hot_encode(input, size):\n",
    "    vec = torch.zeros(size).float()\n",
    "    vec[input] = 1.0\n",
    "    return vec\n",
    "\n",
    "ohe = one_hot_encode(input, size)\n",
    "linear_layer = nn.Linear(size, 1, bias=False)\n",
    "\n",
    "# Set edge weights from 0 to 9 for easy reference\n",
    "with torch.no_grad():\n",
    "    linear_layer.weight = nn.Parameter(\n",
    "        torch.arange(10, dtype=torch.float).reshape(linear_layer.weight.shape))\n",
    "\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer(ohe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af2f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]], requires_grad=True)\n",
      "tensor([3.], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(size, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding_layer.weight = nn.Parameter(\n",
    "        torch.arange(10, dtype=torch.float\n",
    "        ).reshape(embedding_layer.weight.shape))\n",
    "\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor(input)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7617d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Encode input to lower-dimensional representation\n",
    "        hidden = self.embed(input)\n",
    "        # Expand hidden layer to predictions\n",
    "        logits = self.expand(hidden)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d219f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "EMBED_SIZE = 100 # Quite small, just for the tutorial\n",
    "model = Word2Vec(n_v, EMBED_SIZE)\n",
    "\n",
    "# Relevant if you have a GPU:\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define training parameters\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83382a610ccf4b16b445f078f68aabcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(EPOCHS * len(dataloader['train'])))\n",
    "running_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for center, context in dataloader['train']:\n",
    "        center, context = center.to(device), context.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input=context)\n",
    "        loss = loss_fn(logits, center)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "    epoch_loss /= len(dataloader['train'])\n",
    "    running_loss.append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e11218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ace8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvecs = model.expand.weight.cpu().detach().numpy()\n",
    "tokens = ['good', 'father', 'school', 'hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e301d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(wordvecs, metric):\n",
    "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "    return dist_matrix\n",
    "\n",
    "def get_k_similar_words(word, dist_matrix, k=10):\n",
    "    idx = tok2id[word]\n",
    "    dists = dist_matrix[idx]\n",
    "    ind = np.argpartition(dists, k)[:k+1]\n",
    "    ind = ind[np.argsort(dists[ind])][1:]\n",
    "    out = [(i, id2tok[i], dists[i]) for i in ind]\n",
    "    return out\n",
    "\n",
    "dmat = get_distance_matrix(wordvecs, 'cosine')\n",
    "for word in tokens:\n",
    "    print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tok2id['father']\n",
    "plt.plot(wordvecs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 90\n",
    "progress_bar = tqdm(range(EPOCHS * len(dataloader['train'])))\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for center, context in dataloader['train']:\n",
    "        center, context = center.to(device), context.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input=context)\n",
    "        loss = loss_fn(logits, center)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "    epoch_loss /= len(dataloader['train'])\n",
    "    running_loss.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9cf17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9238d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvecs_100_epochs = model.expand.weight.cpu().detach().numpy()\n",
    "dmat_100_epochs = get_distance_matrix(wordvecs_100_epochs, 'cosine')\n",
    "for word in tokens:\n",
    "    print(word, [t[1] for t in get_k_similar_words(word, dmat_100_epochs)], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c821a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ba9bf6",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c044a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
