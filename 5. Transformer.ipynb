{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1dbb0c",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\" width=150px> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>ChatGPT and Friends</h1>\n",
    "<h1>Transformer</h1>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f0117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext;\n",
    "from torchtext import data\n",
    "\n",
    "import spacy\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e633f64",
   "metadata": {},
   "source": [
    "We start by printing out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86582a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.5.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 95a23b8725d520f577ffae3e98b501b423302225\n",
      "\n",
      "spacy     : 3.7.4\n",
      "torch     : 2.3.0\n",
      "tqdm      : 4.66.4\n",
      "watermark : 2.4.3\n",
      "pandas    : 2.1.4\n",
      "networkx  : 3.3\n",
      "torchtext : 0.6.0\n",
      "matplotlib: 3.8.0\n",
      "numpy     : 1.26.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808da77",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a03c84",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c53c94",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3182a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3072a98",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6344441",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834981b",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae370e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant positional encoding matrix\n",
    "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
    "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
    "        seq_len = x.size()[1]\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af1d4d",
   "metadata": {},
   "source": [
    "## Model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de20b4",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention layer\n",
    "\n",
    "![](images/scaled_dot_product_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c96df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given Query, Key, Value, calculate the final weighted value\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
    "    # Shape of v is (batch_size, seq_len, d_v)\n",
    "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Apply mask to scores\n",
    "    # <pad>\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
    "        \n",
    "    # Softmax along the last dimension\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d405d64",
   "metadata": {},
   "source": [
    "### Multi-Head Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b945c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        # self attention linear layers\n",
    "        # Linear layers for q, k, v vectors generation in different heads\n",
    "        self.q_linear_layers = []\n",
    "        self.k_linear_layers = []\n",
    "        self.v_linear_layers = []\n",
    "        for i in range(n_heads):\n",
    "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        multi_head_attention_outputs = []\n",
    "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
    "                                                self.k_linear_layers,\n",
    "                                                self.v_linear_layers):\n",
    "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
    "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
    "            new_v = v_linear(v)  # size: (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Scaled Dot-Product attention\n",
    "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
    "            multi_head_attention_outputs.append(head_v)\n",
    "            \n",
    "        # Concat\n",
    "        #import pdb; pdb.set_trace()\n",
    "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
    "        \n",
    "        # Linear layer to recover to original shap\n",
    "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d2343",
   "metadata": {},
   "source": [
    "### Feed Forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17b121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211156a",
   "metadata": {},
   "source": [
    "### Layer Normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6aaed",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c99ce5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^{2} = \\frac{1}{m} \\sum^{m}_{i=1}(x_{i} - \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b826b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{Z}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma^{2}_{i} + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279adf27",
   "metadata": {},
   "source": [
    "#### Add two learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd71ba6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{Z}_i = \\alpha_i * \\hat{Z}_i + \\beta_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3ea7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len, d_model)\n",
    "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
    "        x_tilde = self.alpha*x_hat + self.beta\n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ff63c",
   "metadata": {},
   "source": [
    "## Encoder & Decoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42ab11",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "An encoder layer contains a multi-head attention layer and feed forward layer\n",
    "\n",
    "![](images/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2deef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        x = x + self.dropout_2(self.feed_forward(x))\n",
    "        x = self.norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfe012",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "An decoder layer contains two multi-head attention layers and one feed forward layer\n",
    "\n",
    "![](images/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d020ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.norm_3 = LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
    "        x = x + self.norm_1(x)\n",
    "        \n",
    "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
    "        x = x + self.norm_2(x)\n",
    "        \n",
    "        x = self.dropout_3(self.feed_forward(x))\n",
    "        x = x + self.norm_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d42ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_layer(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c68f1",
   "metadata": {},
   "source": [
    "## Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb91a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f158758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for decoder in self.decoder_layers:\n",
    "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec3c5f",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "![](images/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ba9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
    "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "        output = self.linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd6d4a",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce235c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f0fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da11599",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(lower=True, tokenize=tokenizer)\n",
    "TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832fb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = open('data/english.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2819e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_data = open('data/french.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a655df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc1f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data, columns=['src', 'trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfe269b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.\\n</td>\n",
       "      <td>Va !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Cours‚ÄØ!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Courez‚ÄØ!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!\\n</td>\n",
       "      <td>Au feu !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!\\n</td>\n",
       "      <td>√Ä l'aide‚ÄØ!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154878</th>\n",
       "      <td>\"Top-down economics never works,\" said Obama. ...</td>\n",
       "      <td>¬´ L'√©conomie en partant du haut vers le bas, √ß...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154879</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154880</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous d√©courage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154881</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154882</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne conna√Æt pas vos ant√©c√©dent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154883 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0                                                   Go.\\n   \n",
       "1                                                  Run!\\n   \n",
       "2                                                  Run!\\n   \n",
       "3                                                 Fire!\\n   \n",
       "4                                                 Help!\\n   \n",
       "...                                                   ...   \n",
       "154878  \"Top-down economics never works,\" said Obama. ...   \n",
       "154879  A carbon footprint is the amount of carbon dio...   \n",
       "154880  Death is something that we're often discourage...   \n",
       "154881  Since there are usually multiple websites on a...   \n",
       "154882  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                      trg  \n",
       "0                                                  Va !\\n  \n",
       "1                                               Cours‚ÄØ!\\n  \n",
       "2                                              Courez‚ÄØ!\\n  \n",
       "3                                              Au feu !\\n  \n",
       "4                                            √Ä l'aide‚ÄØ!\\n  \n",
       "...                                                   ...  \n",
       "154878  ¬´ L'√©conomie en partant du haut vers le bas, √ß...  \n",
       "154879  Une empreinte carbone est la somme de pollutio...  \n",
       "154880  La mort est une chose qu'on nous d√©courage sou...  \n",
       "154881  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "154882  Si quelqu'un qui ne conna√Æt pas vos ant√©c√©dent...  \n",
       "\n",
       "[154883 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12da949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/en_to_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8954aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f93dc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.TabularDataset('data/en_to_fr.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9f40e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f211360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14115"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8761114",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b97f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28354"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bca6c6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x174462b50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b6cfd",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "553a39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "N = 6\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd889365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af71817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/j1bs1q851k15cj5y777nxwph0000gn/T/ipykernel_52672/1058947583.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b59128e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf2c7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train_set, batch_size=32, sort_key=lambda x: (len(x.src), len(x.trg)), shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dce007c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_input, trg_input):\n",
    "    # Source input mask\n",
    "    pad = SRC.vocab.stoi['<pad>']\n",
    "    src_mask = (src_input != pad).unsqueeze(1)\n",
    "    \n",
    "    # Target input mask\n",
    "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
    "    \n",
    "    seq_len = trg_input.size(1)\n",
    "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
    "    trg_mask = trg_mask & nopeak_mask\n",
    "    \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b80febc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, output_interval=100):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, batch in tqdm(enumerate(train_iter)):\n",
    "            \n",
    "            src_input = batch.src.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            trg = batch.trg.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create src & trg masks\n",
    "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
    "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i + 1) % output_interval == 0:\n",
    "                avg_loss = total_loss/output_interval\n",
    "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
    "                                                                           epoch + 1,\n",
    "                                                                           i + 1,\n",
    "                                                                           avg_loss))\n",
    "                total_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4f743df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f976dff605c048718a6f5f9a83ff9516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.08011873960494995, epoch = 1, iter = 10, loss = 9.78813648223877\n",
      "time = 0.07106265226999918, epoch = 1, iter = 20, loss = 9.154868793487548\n",
      "time = 0.07787988583246867, epoch = 1, iter = 30, loss = 8.607198905944824\n",
      "time = 0.08362374703089397, epoch = 1, iter = 40, loss = 8.000648689270019\n",
      "time = 0.0727192521095276, epoch = 1, iter = 50, loss = 7.483107423782348\n",
      "time = 0.09928426742553711, epoch = 1, iter = 60, loss = 6.991530990600586\n",
      "time = 0.11268201271692911, epoch = 1, iter = 70, loss = 6.626857280731201\n",
      "time = 0.18360661665598552, epoch = 1, iter = 80, loss = 6.372199487686157\n",
      "time = 0.10917487939198812, epoch = 1, iter = 90, loss = 6.218957710266113\n",
      "time = 0.11455166737238566, epoch = 1, iter = 100, loss = 6.144716453552246\n",
      "time = 0.12103782097498576, epoch = 1, iter = 110, loss = 6.159973573684693\n",
      "time = 0.14533424774805706, epoch = 1, iter = 120, loss = 6.073071050643921\n",
      "time = 0.08959454695383708, epoch = 1, iter = 130, loss = 6.049617195129395\n",
      "time = 0.09605932235717773, epoch = 1, iter = 140, loss = 6.045978260040283\n",
      "time = 0.08898904720942179, epoch = 1, iter = 150, loss = 6.092438364028931\n",
      "time = 0.1043753465016683, epoch = 1, iter = 160, loss = 6.049756765365601\n",
      "time = 0.087066650390625, epoch = 1, iter = 170, loss = 6.02962293624878\n",
      "time = 0.07234763701756795, epoch = 1, iter = 180, loss = 6.100237798690796\n",
      "time = 0.09365872144699097, epoch = 1, iter = 190, loss = 6.04655704498291\n",
      "time = 0.0974254329999288, epoch = 1, iter = 200, loss = 6.002943992614746\n",
      "time = 0.100044318040212, epoch = 1, iter = 210, loss = 6.075710296630859\n",
      "time = 0.11440418163935344, epoch = 1, iter = 220, loss = 6.106454849243164\n",
      "time = 0.16544063091278077, epoch = 1, iter = 230, loss = 6.054590559005737\n",
      "time = 0.1065251628557841, epoch = 1, iter = 240, loss = 6.0816723823547365\n",
      "time = 0.10022213061650594, epoch = 1, iter = 250, loss = 6.0525171756744385\n",
      "time = 0.07868516445159912, epoch = 1, iter = 260, loss = 6.049773740768432\n",
      "time = 0.07897488673528036, epoch = 1, iter = 270, loss = 6.005603933334351\n",
      "time = 0.10137176513671875, epoch = 1, iter = 280, loss = 6.054307651519776\n",
      "time = 0.07626307010650635, epoch = 1, iter = 290, loss = 6.011839485168457\n",
      "time = 0.08472275336583455, epoch = 1, iter = 300, loss = 5.9788398265838625\n",
      "time = 0.07742636601130168, epoch = 1, iter = 310, loss = 6.038734722137451\n",
      "time = 0.08254598379135132, epoch = 1, iter = 320, loss = 5.979532670974732\n",
      "time = 0.09682366847991944, epoch = 1, iter = 330, loss = 6.0100486278533936\n",
      "time = 0.09971574942270915, epoch = 1, iter = 340, loss = 6.046740484237671\n",
      "time = 0.09925861358642578, epoch = 1, iter = 350, loss = 5.988676452636719\n",
      "time = 0.16178710063298543, epoch = 1, iter = 360, loss = 6.039321231842041\n",
      "time = 0.08811784982681274, epoch = 1, iter = 370, loss = 6.059715032577515\n",
      "time = 0.07502776781717936, epoch = 1, iter = 380, loss = 6.052368593215943\n",
      "time = 0.09027719895044962, epoch = 1, iter = 390, loss = 5.9371482849121096\n",
      "time = 0.09089048306147257, epoch = 1, iter = 400, loss = 6.051039361953736\n",
      "time = 0.0967425028483073, epoch = 1, iter = 410, loss = 5.935706758499146\n",
      "time = 0.12018518050511678, epoch = 1, iter = 420, loss = 5.994293260574341\n",
      "time = 0.1031469980875651, epoch = 1, iter = 430, loss = 6.067947340011597\n",
      "time = 0.09077063401540121, epoch = 1, iter = 440, loss = 6.0602621078491214\n",
      "time = 0.12478878100713094, epoch = 1, iter = 450, loss = 5.955132532119751\n",
      "time = 0.14016460180282592, epoch = 1, iter = 460, loss = 5.989595699310303\n",
      "time = 0.1434977134068807, epoch = 1, iter = 470, loss = 6.011815166473388\n",
      "time = 0.09198988278706868, epoch = 1, iter = 480, loss = 6.051754760742187\n",
      "time = 0.08385519981384278, epoch = 1, iter = 490, loss = 6.037171506881714\n",
      "time = 0.10386543273925782, epoch = 1, iter = 500, loss = 6.017087507247925\n",
      "time = 0.08036566575368245, epoch = 1, iter = 510, loss = 5.913239240646362\n",
      "time = 0.09375406901041666, epoch = 1, iter = 520, loss = 6.063561296463012\n",
      "time = 0.09815526803334554, epoch = 1, iter = 530, loss = 6.004400730133057\n",
      "time = 0.08904250065485636, epoch = 1, iter = 540, loss = 6.017862796783447\n",
      "time = 0.08239713509877523, epoch = 1, iter = 550, loss = 5.937914180755615\n",
      "time = 0.1434497316678365, epoch = 1, iter = 560, loss = 6.010753440856933\n",
      "time = 0.10849331617355347, epoch = 1, iter = 570, loss = 5.976103734970093\n",
      "time = 0.16047430038452148, epoch = 1, iter = 580, loss = 6.001987075805664\n",
      "time = 0.07837321360905965, epoch = 1, iter = 590, loss = 6.018808841705322\n",
      "time = 0.09410820404688518, epoch = 1, iter = 600, loss = 6.1312964916229244\n",
      "time = 0.1009464184443156, epoch = 1, iter = 610, loss = 5.936617088317871\n",
      "time = 0.08265153566996257, epoch = 1, iter = 620, loss = 6.018870830535889\n",
      "time = 0.11060029665629069, epoch = 1, iter = 630, loss = 6.0322753429412845\n",
      "time = 0.07928319772084554, epoch = 1, iter = 640, loss = 5.925721502304077\n",
      "time = 0.08127316236495971, epoch = 1, iter = 650, loss = 6.0486548900604244\n",
      "time = 0.08720424969991049, epoch = 1, iter = 660, loss = 5.997279357910156\n",
      "time = 0.14387425184249877, epoch = 1, iter = 670, loss = 6.029330778121948\n",
      "time = 0.11748611927032471, epoch = 1, iter = 680, loss = 5.994606781005859\n",
      "time = 0.14836701552073162, epoch = 1, iter = 690, loss = 6.051019954681396\n",
      "time = 0.0954910675684611, epoch = 1, iter = 700, loss = 6.088670492172241\n",
      "time = 0.09359125296274821, epoch = 1, iter = 710, loss = 6.0425950050354\n",
      "time = 0.08091708024342854, epoch = 1, iter = 720, loss = 5.995092535018921\n",
      "time = 0.11091155211130778, epoch = 1, iter = 730, loss = 6.019153308868408\n",
      "time = 0.08393463293711344, epoch = 1, iter = 740, loss = 6.040694761276245\n",
      "time = 0.08126122951507568, epoch = 1, iter = 750, loss = 6.0600501537323\n",
      "time = 0.07462371587753296, epoch = 1, iter = 760, loss = 6.042242956161499\n",
      "time = 0.07902443011601766, epoch = 1, iter = 770, loss = 6.043393325805664\n",
      "time = 0.09138265053431192, epoch = 1, iter = 780, loss = 6.010515832901001\n",
      "time = 0.10584551890691121, epoch = 1, iter = 790, loss = 6.030967950820923\n",
      "time = 0.10659903287887573, epoch = 1, iter = 800, loss = 6.035824251174927\n",
      "time = 0.16963347991307576, epoch = 1, iter = 810, loss = 5.9953619003295895\n",
      "time = 0.09397147099177043, epoch = 1, iter = 820, loss = 6.077472925186157\n",
      "time = 0.10104496876398722, epoch = 1, iter = 830, loss = 6.091148090362549\n",
      "time = 0.10065584977467855, epoch = 1, iter = 840, loss = 6.072861433029175\n",
      "time = 0.10025408665339151, epoch = 1, iter = 850, loss = 6.043430614471435\n",
      "time = 0.07959535121917724, epoch = 1, iter = 860, loss = 5.9641053676605225\n",
      "time = 0.10537628730138143, epoch = 1, iter = 870, loss = 5.96609148979187\n",
      "time = 0.08379099766413371, epoch = 1, iter = 880, loss = 6.029144763946533\n",
      "time = 0.10078396797180175, epoch = 1, iter = 890, loss = 6.00061297416687\n",
      "time = 0.1042209506034851, epoch = 1, iter = 900, loss = 6.027824783325196\n",
      "time = 0.13115118741989135, epoch = 1, iter = 910, loss = 5.9667637825012205\n",
      "time = 0.15706048409144083, epoch = 1, iter = 920, loss = 6.052765321731568\n",
      "time = 0.10412973165512085, epoch = 1, iter = 930, loss = 6.029544115066528\n",
      "time = 0.07724536657333374, epoch = 1, iter = 940, loss = 5.977599382400513\n",
      "time = 0.09842636982599894, epoch = 1, iter = 950, loss = 6.022625350952149\n",
      "time = 0.09234496355056762, epoch = 1, iter = 960, loss = 6.011456394195557\n",
      "time = 0.08081330060958862, epoch = 1, iter = 970, loss = 5.9508133888244625\n",
      "time = 0.08496263027191162, epoch = 1, iter = 980, loss = 6.014390468597412\n",
      "time = 0.09012968142827352, epoch = 1, iter = 990, loss = 6.019685077667236\n",
      "time = 0.0871908982594808, epoch = 1, iter = 1000, loss = 6.075190639495849\n",
      "time = 0.08965561787287395, epoch = 1, iter = 1010, loss = 6.026991510391236\n",
      "time = 0.16229368448257447, epoch = 1, iter = 1020, loss = 6.003897476196289\n",
      "time = 0.14151808023452758, epoch = 1, iter = 1030, loss = 5.925416660308838\n",
      "time = 0.11449365218480428, epoch = 1, iter = 1040, loss = 6.07681975364685\n",
      "time = 0.10179638465245565, epoch = 1, iter = 1050, loss = 6.052192306518554\n",
      "time = 0.09798648357391357, epoch = 1, iter = 1060, loss = 6.061021709442139\n",
      "time = 0.08749866882960002, epoch = 1, iter = 1070, loss = 5.985856437683106\n",
      "time = 0.0986311674118042, epoch = 1, iter = 1080, loss = 6.0798420906066895\n",
      "time = 0.10859604676564534, epoch = 1, iter = 1090, loss = 6.034544801712036\n",
      "time = 0.08643253246943156, epoch = 1, iter = 1100, loss = 6.017837619781494\n",
      "time = 0.09273016850153605, epoch = 1, iter = 1110, loss = 5.956553602218628\n",
      "time = 0.13618738253911336, epoch = 1, iter = 1120, loss = 6.09350175857544\n",
      "time = 0.12212308645248413, epoch = 1, iter = 1130, loss = 6.030484056472778\n",
      "time = 0.14616944789886474, epoch = 1, iter = 1140, loss = 5.996807622909546\n",
      "time = 0.09711215098698935, epoch = 1, iter = 1150, loss = 6.003525114059448\n",
      "time = 0.09160298109054565, epoch = 1, iter = 1160, loss = 5.997168397903442\n",
      "time = 0.11157968839009604, epoch = 1, iter = 1170, loss = 6.081601572036743\n",
      "time = 0.10831143458684285, epoch = 1, iter = 1180, loss = 5.9248566150665285\n",
      "time = 0.08811034758885701, epoch = 1, iter = 1190, loss = 5.973328447341919\n",
      "time = 0.11190906763076783, epoch = 1, iter = 1200, loss = 5.978841304779053\n",
      "time = 0.10593721866607667, epoch = 1, iter = 1210, loss = 6.106311225891114\n",
      "time = 0.08578361670176188, epoch = 1, iter = 1220, loss = 6.050134181976318\n",
      "time = 0.10799420277277628, epoch = 1, iter = 1230, loss = 5.920544672012329\n",
      "time = 0.12088686227798462, epoch = 1, iter = 1240, loss = 6.004422855377197\n",
      "time = 0.1650460163752238, epoch = 1, iter = 1250, loss = 5.968671464920044\n",
      "time = 0.10042614936828613, epoch = 1, iter = 1260, loss = 6.068270254135132\n",
      "time = 0.08991631666819254, epoch = 1, iter = 1270, loss = 6.01930046081543\n",
      "time = 0.1133439024289449, epoch = 1, iter = 1280, loss = 6.014482498168945\n",
      "time = 0.0930912454922994, epoch = 1, iter = 1290, loss = 5.981867027282715\n",
      "time = 0.10992040236790974, epoch = 1, iter = 1300, loss = 5.997107648849488\n",
      "time = 0.09759434858957926, epoch = 1, iter = 1310, loss = 6.017019128799438\n",
      "time = 0.09309237798055013, epoch = 1, iter = 1320, loss = 6.054151153564453\n",
      "time = 0.11974271933237711, epoch = 1, iter = 1330, loss = 5.9726707458496096\n",
      "time = 0.12641008297602335, epoch = 1, iter = 1340, loss = 5.927011585235595\n",
      "time = 0.15137759844462076, epoch = 1, iter = 1350, loss = 5.981242752075195\n",
      "time = 0.115467369556427, epoch = 1, iter = 1360, loss = 6.009082794189453\n",
      "time = 0.09430380264918009, epoch = 1, iter = 1370, loss = 6.026061105728149\n",
      "time = 0.09485956827799479, epoch = 1, iter = 1380, loss = 5.985097217559814\n",
      "time = 0.08356070121129354, epoch = 1, iter = 1390, loss = 5.933017635345459\n",
      "time = 0.11275408665339152, epoch = 1, iter = 1400, loss = 5.899556493759155\n",
      "time = 0.08854955037434896, epoch = 1, iter = 1410, loss = 5.904842567443848\n",
      "time = 0.09620646238327027, epoch = 1, iter = 1420, loss = 5.909442806243897\n",
      "time = 0.08570719957351684, epoch = 1, iter = 1430, loss = 5.95278582572937\n",
      "time = 0.11123379866282145, epoch = 1, iter = 1440, loss = 5.968101453781128\n",
      "time = 0.13530052900314332, epoch = 1, iter = 1450, loss = 6.070425510406494\n",
      "time = 0.16438330014546712, epoch = 1, iter = 1460, loss = 5.8904341697692875\n",
      "time = 0.07805776596069336, epoch = 1, iter = 1470, loss = 5.932385873794556\n",
      "time = 0.09766670068105061, epoch = 1, iter = 1480, loss = 5.926341485977173\n",
      "time = 0.081975785891215, epoch = 1, iter = 1490, loss = 5.865867662429809\n",
      "time = 0.07969379822413127, epoch = 1, iter = 1500, loss = 5.871694564819336\n",
      "time = 0.11185973087946574, epoch = 1, iter = 1510, loss = 5.929322004318237\n",
      "time = 0.10499240159988403, epoch = 1, iter = 1520, loss = 5.918481731414795\n",
      "time = 0.08830146392186483, epoch = 1, iter = 1530, loss = 5.944946002960205\n",
      "time = 0.09152203798294067, epoch = 1, iter = 1540, loss = 6.019783926010132\n",
      "time = 0.1222117026646932, epoch = 1, iter = 1550, loss = 5.928389215469361\n",
      "time = 0.1316712498664856, epoch = 1, iter = 1560, loss = 5.960775232315063\n",
      "time = 0.13018754720687867, epoch = 1, iter = 1570, loss = 6.00138840675354\n",
      "time = 0.11880995035171509, epoch = 1, iter = 1580, loss = 5.962071895599365\n",
      "time = 0.07825435400009155, epoch = 1, iter = 1590, loss = 5.944967031478882\n",
      "time = 0.10749099651972453, epoch = 1, iter = 1600, loss = 5.776764822006226\n",
      "time = 0.09051116704940795, epoch = 1, iter = 1610, loss = 5.870401000976562\n",
      "time = 0.07819174528121949, epoch = 1, iter = 1620, loss = 5.920604276657104\n",
      "time = 0.10116471846898396, epoch = 1, iter = 1630, loss = 5.893030309677124\n",
      "time = 0.12029469807942708, epoch = 1, iter = 1640, loss = 5.89957070350647\n",
      "time = 0.08599925438563029, epoch = 1, iter = 1650, loss = 5.89238862991333\n",
      "time = 0.0924332857131958, epoch = 1, iter = 1660, loss = 5.933668422698974\n",
      "time = 0.13250956535339356, epoch = 1, iter = 1670, loss = 5.971982526779175\n",
      "time = 0.18759994904200236, epoch = 1, iter = 1680, loss = 5.992204761505127\n",
      "time = 0.11507873137791952, epoch = 1, iter = 1690, loss = 5.829189252853394\n",
      "time = 0.08922101656595866, epoch = 1, iter = 1700, loss = 5.799696254730224\n",
      "time = 0.08949232896169027, epoch = 1, iter = 1710, loss = 5.866227817535401\n",
      "time = 0.1007408340771993, epoch = 1, iter = 1720, loss = 5.871028757095337\n",
      "time = 0.11842931509017944, epoch = 1, iter = 1730, loss = 5.948229694366455\n",
      "time = 0.09724452098210652, epoch = 1, iter = 1740, loss = 5.936162614822388\n",
      "time = 0.1037877639134725, epoch = 1, iter = 1750, loss = 5.9115965366363525\n",
      "time = 0.09993119637171427, epoch = 1, iter = 1760, loss = 5.976129961013794\n",
      "time = 0.09896339972813924, epoch = 1, iter = 1770, loss = 5.884162759780883\n",
      "time = 0.11180508534113566, epoch = 1, iter = 1780, loss = 5.920590782165528\n",
      "time = 0.13872130314509074, epoch = 1, iter = 1790, loss = 5.885991621017456\n",
      "time = 0.12622665166854857, epoch = 1, iter = 1800, loss = 5.975068092346191\n",
      "time = 0.07854870557785035, epoch = 1, iter = 1810, loss = 5.8704711437225345\n",
      "time = 0.10162811676661174, epoch = 1, iter = 1820, loss = 5.905290699005127\n",
      "time = 0.08077549934387207, epoch = 1, iter = 1830, loss = 5.9651562690734865\n",
      "time = 0.09896884759267172, epoch = 1, iter = 1840, loss = 5.948011350631714\n",
      "time = 0.0865380843480428, epoch = 1, iter = 1850, loss = 5.881498003005982\n",
      "time = 0.09108478228251139, epoch = 1, iter = 1860, loss = 5.781168842315674\n",
      "time = 0.08076063791910808, epoch = 1, iter = 1870, loss = 5.799612426757813\n",
      "time = 0.1072535514831543, epoch = 1, iter = 1880, loss = 5.781299591064453\n",
      "time = 0.1263908823331197, epoch = 1, iter = 1890, loss = 5.887020683288574\n",
      "time = 0.1323433001836141, epoch = 1, iter = 1900, loss = 5.803486490249634\n",
      "time = 0.1357654809951782, epoch = 1, iter = 1910, loss = 5.8775452136993405\n",
      "time = 0.1099696159362793, epoch = 1, iter = 1920, loss = 5.913744497299194\n",
      "time = 0.09609164794286092, epoch = 1, iter = 1930, loss = 5.729432582855225\n",
      "time = 0.1136424700419108, epoch = 1, iter = 1940, loss = 5.874795293807983\n",
      "time = 0.08773453632990519, epoch = 1, iter = 1950, loss = 5.814392137527466\n",
      "time = 0.08897201220194499, epoch = 1, iter = 1960, loss = 5.819191551208496\n",
      "time = 0.09286591609319052, epoch = 1, iter = 1970, loss = 5.954627466201782\n",
      "time = 0.11556516885757447, epoch = 1, iter = 1980, loss = 5.832953023910522\n",
      "time = 0.1190809686978658, epoch = 1, iter = 1990, loss = 5.876195049285888\n",
      "time = 0.09830448627471924, epoch = 1, iter = 2000, loss = 5.856702613830566\n",
      "time = 0.1563976526260376, epoch = 1, iter = 2010, loss = 5.852602958679199\n",
      "time = 0.09459648132324219, epoch = 1, iter = 2020, loss = 5.8575060844421385\n",
      "time = 0.08974781831105551, epoch = 1, iter = 2030, loss = 5.755156421661377\n",
      "time = 0.09356646537780762, epoch = 1, iter = 2040, loss = 5.824849796295166\n",
      "time = 0.09195324977238974, epoch = 1, iter = 2050, loss = 5.743112707138062\n",
      "time = 0.08595099449157714, epoch = 1, iter = 2060, loss = 5.834292030334472\n",
      "time = 0.1052830696105957, epoch = 1, iter = 2070, loss = 5.849585294723511\n",
      "time = 0.08196166753768921, epoch = 1, iter = 2080, loss = 5.803666687011718\n",
      "time = 0.09792954921722412, epoch = 1, iter = 2090, loss = 5.825573968887329\n",
      "time = 0.09161970218022665, epoch = 1, iter = 2100, loss = 5.782606315612793\n",
      "time = 0.12918355067571005, epoch = 1, iter = 2110, loss = 5.8541741371154785\n",
      "time = 0.1394402782122294, epoch = 1, iter = 2120, loss = 5.809002113342285\n",
      "time = 0.13986276785532634, epoch = 1, iter = 2130, loss = 5.8581202030181885\n",
      "time = 0.10397423505783081, epoch = 1, iter = 2140, loss = 5.83325080871582\n",
      "time = 0.09184716939926148, epoch = 1, iter = 2150, loss = 5.721041059494018\n",
      "time = 0.11894530455271403, epoch = 1, iter = 2160, loss = 5.792725133895874\n",
      "time = 0.09629667997360229, epoch = 1, iter = 2170, loss = 5.810942363739014\n",
      "time = 0.11728545427322387, epoch = 1, iter = 2180, loss = 5.842166137695313\n",
      "time = 0.09934504826863606, epoch = 1, iter = 2190, loss = 5.868635463714599\n",
      "time = 0.10462525288263956, epoch = 1, iter = 2200, loss = 5.798760414123535\n",
      "time = 0.13384151856104534, epoch = 1, iter = 2210, loss = 5.829855585098267\n",
      "time = 0.1289340813954671, epoch = 1, iter = 2220, loss = 5.716730403900146\n",
      "time = 0.1623010516166687, epoch = 1, iter = 2230, loss = 5.957237339019775\n",
      "time = 0.08081373373667398, epoch = 1, iter = 2240, loss = 5.848803949356079\n",
      "time = 0.08640329837799073, epoch = 1, iter = 2250, loss = 5.844105863571167\n",
      "time = 0.08697301149368286, epoch = 1, iter = 2260, loss = 5.863369274139404\n",
      "time = 0.10305271943410238, epoch = 1, iter = 2270, loss = 5.77318639755249\n",
      "time = 0.10037158330281576, epoch = 1, iter = 2280, loss = 5.822571897506714\n",
      "time = 0.1024948517481486, epoch = 1, iter = 2290, loss = 5.746425342559815\n",
      "time = 0.12935816446940104, epoch = 1, iter = 2300, loss = 5.882665729522705\n",
      "time = 0.11154595216115316, epoch = 1, iter = 2310, loss = 5.699743986129761\n",
      "time = 0.13961678345998127, epoch = 1, iter = 2320, loss = 5.8405107975006105\n",
      "time = 0.16584153175354005, epoch = 1, iter = 2330, loss = 5.801825475692749\n",
      "time = 0.08335813283920288, epoch = 1, iter = 2340, loss = 5.752759695053101\n",
      "time = 0.08819032907485962, epoch = 1, iter = 2350, loss = 5.818166303634643\n",
      "time = 0.08895198504130046, epoch = 1, iter = 2360, loss = 5.9014753818511965\n",
      "time = 0.0929190993309021, epoch = 1, iter = 2370, loss = 5.807371807098389\n",
      "time = 0.10785033305486043, epoch = 1, iter = 2380, loss = 5.723461103439331\n",
      "time = 0.07923093239466349, epoch = 1, iter = 2390, loss = 5.8431251525878904\n",
      "time = 0.07865417003631592, epoch = 1, iter = 2400, loss = 5.918694734573364\n",
      "time = 0.10563264687856039, epoch = 1, iter = 2410, loss = 5.8088665962219235\n",
      "time = 0.1128811796506246, epoch = 1, iter = 2420, loss = 5.774657487869263\n",
      "time = 0.09469895362854004, epoch = 1, iter = 2430, loss = 5.7785145282745365\n",
      "time = 0.09588104883829753, epoch = 1, iter = 2440, loss = 5.763167047500611\n",
      "time = 0.16446528434753419, epoch = 1, iter = 2450, loss = 5.80375862121582\n",
      "time = 0.09092308282852173, epoch = 1, iter = 2460, loss = 5.850852012634277\n",
      "time = 0.07770918210347494, epoch = 1, iter = 2470, loss = 5.874308061599732\n",
      "time = 0.09451541900634766, epoch = 1, iter = 2480, loss = 5.781949663162232\n",
      "time = 0.09554540316263835, epoch = 1, iter = 2490, loss = 5.880513095855713\n",
      "time = 0.10643278360366822, epoch = 1, iter = 2500, loss = 5.782290887832642\n",
      "time = 0.08428046305974325, epoch = 1, iter = 2510, loss = 5.737268590927124\n",
      "time = 0.10766424735387166, epoch = 1, iter = 2520, loss = 5.741582632064819\n",
      "time = 0.09447798728942872, epoch = 1, iter = 2530, loss = 5.814034461975098\n",
      "time = 0.13743171691894532, epoch = 1, iter = 2540, loss = 5.788821887969971\n",
      "time = 0.11598789691925049, epoch = 1, iter = 2550, loss = 5.789792680740357\n",
      "time = 0.14729175170262654, epoch = 1, iter = 2560, loss = 5.706040239334106\n",
      "time = 0.09373193581899007, epoch = 1, iter = 2570, loss = 5.832785940170288\n",
      "time = 0.10368391672770182, epoch = 1, iter = 2580, loss = 5.777712059020996\n",
      "time = 0.0848332683245341, epoch = 1, iter = 2590, loss = 5.829152965545655\n",
      "time = 0.09673225084940593, epoch = 1, iter = 2600, loss = 5.668253612518311\n",
      "time = 0.0944697658220927, epoch = 1, iter = 2610, loss = 5.810303211212158\n",
      "time = 0.08554883003234863, epoch = 1, iter = 2620, loss = 5.773363161087036\n",
      "time = 0.0861763040224711, epoch = 1, iter = 2630, loss = 5.8027771472930905\n",
      "time = 0.09947368701299032, epoch = 1, iter = 2640, loss = 5.769598293304443\n",
      "time = 0.09219258626302083, epoch = 1, iter = 2650, loss = 5.830829668045044\n",
      "time = 0.12756696939468384, epoch = 1, iter = 2660, loss = 5.837671995162964\n",
      "time = 0.1767883022626241, epoch = 1, iter = 2670, loss = 5.8960987567901615\n",
      "time = 0.08050101598103841, epoch = 1, iter = 2680, loss = 5.768973255157471\n",
      "time = 0.09547032117843628, epoch = 1, iter = 2690, loss = 5.759123420715332\n",
      "time = 0.09165088335673015, epoch = 1, iter = 2700, loss = 5.8552086353302\n",
      "time = 0.09826761484146118, epoch = 1, iter = 2710, loss = 5.844935607910156\n",
      "time = 0.09057125250498453, epoch = 1, iter = 2720, loss = 5.821252727508545\n",
      "time = 0.07953078349431356, epoch = 1, iter = 2730, loss = 5.668329238891602\n",
      "time = 0.07870988448460897, epoch = 1, iter = 2740, loss = 5.70167293548584\n",
      "time = 0.10940223534901937, epoch = 1, iter = 2750, loss = 5.751743936538697\n",
      "time = 0.11254558165868124, epoch = 1, iter = 2760, loss = 5.745285844802856\n",
      "time = 0.1439410130182902, epoch = 1, iter = 2770, loss = 5.849240016937256\n",
      "time = 0.13848136663436889, epoch = 1, iter = 2780, loss = 5.780015754699707\n",
      "time = 0.11900511582692465, epoch = 1, iter = 2790, loss = 5.847562599182129\n",
      "time = 0.0979122519493103, epoch = 1, iter = 2800, loss = 5.7552042484283445\n",
      "time = 0.09041471481323242, epoch = 1, iter = 2810, loss = 5.791794157028198\n",
      "time = 0.09328231811523438, epoch = 1, iter = 2820, loss = 5.79313554763794\n",
      "time = 0.0879939874013265, epoch = 1, iter = 2830, loss = 5.7554949760437015\n",
      "time = 0.09006489912668864, epoch = 1, iter = 2840, loss = 5.664337491989135\n",
      "time = 0.09725797971089681, epoch = 1, iter = 2850, loss = 5.795121479034424\n",
      "time = 0.10783584912618001, epoch = 1, iter = 2860, loss = 5.783633708953857\n",
      "time = 0.10490431785583496, epoch = 1, iter = 2870, loss = 5.764722919464111\n",
      "time = 0.11912436485290527, epoch = 1, iter = 2880, loss = 5.777744197845459\n",
      "time = 0.1651144027709961, epoch = 1, iter = 2890, loss = 5.819845676422119\n",
      "time = 0.12496968507766723, epoch = 1, iter = 2900, loss = 5.731693458557129\n",
      "time = 0.10080860058466594, epoch = 1, iter = 2910, loss = 5.768958377838135\n",
      "time = 0.09624760150909424, epoch = 1, iter = 2920, loss = 5.825038862228394\n",
      "time = 0.08828131755193075, epoch = 1, iter = 2930, loss = 5.7707682132720945\n",
      "time = 0.09791965087254842, epoch = 1, iter = 2940, loss = 5.799468946456909\n",
      "time = 0.09570635159810384, epoch = 1, iter = 2950, loss = 5.789903545379639\n",
      "time = 0.09844163656234742, epoch = 1, iter = 2960, loss = 5.879449367523193\n",
      "time = 0.09657106796900432, epoch = 1, iter = 2970, loss = 5.739361715316773\n",
      "time = 0.10950777133305868, epoch = 1, iter = 2980, loss = 5.771697902679444\n",
      "time = 0.1278253475824992, epoch = 1, iter = 2990, loss = 5.759814929962158\n",
      "time = 0.1424086332321167, epoch = 1, iter = 3000, loss = 5.73962173461914\n",
      "time = 0.11494565010070801, epoch = 1, iter = 3010, loss = 5.817432451248169\n",
      "time = 0.07931036551793416, epoch = 1, iter = 3020, loss = 5.757172536849976\n",
      "time = 0.09601484934488932, epoch = 1, iter = 3030, loss = 5.841392135620117\n",
      "time = 0.09389000336329142, epoch = 1, iter = 3040, loss = 5.834493827819824\n",
      "time = 0.08745328187942505, epoch = 1, iter = 3050, loss = 5.754469871520996\n",
      "time = 0.10761781930923461, epoch = 1, iter = 3060, loss = 5.737847757339478\n",
      "time = 0.10716650088628134, epoch = 1, iter = 3070, loss = 5.702453565597534\n",
      "time = 0.09431049823760987, epoch = 1, iter = 3080, loss = 5.828609323501587\n",
      "time = 0.09022133350372315, epoch = 1, iter = 3090, loss = 5.799932765960693\n",
      "time = 0.10719653765360514, epoch = 1, iter = 3100, loss = 5.6455179214477536\n",
      "time = 0.1207051157951355, epoch = 1, iter = 3110, loss = 5.7581196308135985\n",
      "time = 0.15132243633270265, epoch = 1, iter = 3120, loss = 5.851672840118408\n",
      "time = 0.08973653316497802, epoch = 1, iter = 3130, loss = 5.732063436508179\n",
      "time = 0.09279454946517944, epoch = 1, iter = 3140, loss = 5.728740644454956\n",
      "time = 0.10307455062866211, epoch = 1, iter = 3150, loss = 5.690514755249024\n",
      "time = 0.08933807214101155, epoch = 1, iter = 3160, loss = 5.785455846786499\n",
      "time = 0.08135343392690023, epoch = 1, iter = 3170, loss = 5.7749732494354244\n",
      "time = 0.10330496629079183, epoch = 1, iter = 3180, loss = 5.71566481590271\n",
      "time = 0.08261608282725016, epoch = 1, iter = 3190, loss = 5.772892808914184\n",
      "time = 0.08506195147832235, epoch = 1, iter = 3200, loss = 5.707777070999145\n",
      "time = 0.09260466496149698, epoch = 1, iter = 3210, loss = 5.71222186088562\n",
      "time = 0.10515048503875732, epoch = 1, iter = 3220, loss = 5.699247550964356\n",
      "time = 0.1627032478650411, epoch = 1, iter = 3230, loss = 5.6248612880706785\n",
      "time = 0.11211231549580893, epoch = 1, iter = 3240, loss = 5.7045516014099125\n",
      "time = 0.09152518510818482, epoch = 1, iter = 3250, loss = 5.710575437545776\n",
      "time = 0.08680680195490519, epoch = 1, iter = 3260, loss = 5.7729771614074705\n",
      "time = 0.10681205590566, epoch = 1, iter = 3270, loss = 5.885521936416626\n",
      "time = 0.0962267835934957, epoch = 1, iter = 3280, loss = 5.652656698226929\n",
      "time = 0.09520013332366943, epoch = 1, iter = 3290, loss = 5.718860721588134\n",
      "time = 0.10488307873408, epoch = 1, iter = 3300, loss = 5.70621075630188\n",
      "time = 0.10357746680577597, epoch = 1, iter = 3310, loss = 5.715045595169068\n",
      "time = 0.10146404504776001, epoch = 1, iter = 3320, loss = 5.670224237442016\n",
      "time = 0.13708966970443726, epoch = 1, iter = 3330, loss = 5.643611574172974\n",
      "time = 0.1385805368423462, epoch = 1, iter = 3340, loss = 5.596551895141602\n",
      "time = 0.10605003436406453, epoch = 1, iter = 3350, loss = 5.737824010848999\n",
      "time = 0.09370026588439942, epoch = 1, iter = 3360, loss = 5.726261186599731\n",
      "time = 0.1206547498703003, epoch = 1, iter = 3370, loss = 5.611865282058716\n",
      "time = 0.10610450108846028, epoch = 1, iter = 3380, loss = 5.5777651309967045\n",
      "time = 0.08841685056686402, epoch = 1, iter = 3390, loss = 5.688187456130981\n",
      "time = 0.10345250368118286, epoch = 1, iter = 3400, loss = 5.669371032714844\n",
      "time = 0.09272454977035523, epoch = 1, iter = 3410, loss = 5.648877382278442\n",
      "time = 0.09050091505050659, epoch = 1, iter = 3420, loss = 5.71278977394104\n",
      "time = 0.10830753246943156, epoch = 1, iter = 3430, loss = 5.741162633895874\n",
      "time = 0.12411787907282511, epoch = 1, iter = 3440, loss = 5.635964441299438\n",
      "time = 0.17358373006184896, epoch = 1, iter = 3450, loss = 5.678249359130859\n",
      "time = 0.07983160018920898, epoch = 1, iter = 3460, loss = 5.5502471923828125\n",
      "time = 0.08963405291239421, epoch = 1, iter = 3470, loss = 5.60750560760498\n",
      "time = 0.08411876360575359, epoch = 1, iter = 3480, loss = 5.603553056716919\n",
      "time = 0.08426661888758341, epoch = 1, iter = 3490, loss = 5.703389406204224\n",
      "time = 0.09051941633224488, epoch = 1, iter = 3500, loss = 5.655363512039185\n",
      "time = 0.09309423367182414, epoch = 1, iter = 3510, loss = 5.632612705230713\n",
      "time = 0.07979032993316651, epoch = 1, iter = 3520, loss = 5.5583882331848145\n",
      "time = 0.07866998116175333, epoch = 1, iter = 3530, loss = 5.575819730758667\n",
      "time = 0.08683213392893473, epoch = 1, iter = 3540, loss = 5.62414960861206\n",
      "time = 0.10465688308080037, epoch = 1, iter = 3550, loss = 5.6318388938903805\n",
      "time = 0.14258261919021606, epoch = 1, iter = 3560, loss = 5.62956018447876\n",
      "time = 0.1548445463180542, epoch = 1, iter = 3570, loss = 5.595637559890747\n",
      "time = 0.09615735212961833, epoch = 1, iter = 3580, loss = 5.554247951507568\n",
      "time = 0.0888090173403422, epoch = 1, iter = 3590, loss = 5.619170236587524\n",
      "time = 0.09897331794102987, epoch = 1, iter = 3600, loss = 5.63342924118042\n",
      "time = 0.11376325289408366, epoch = 1, iter = 3610, loss = 5.592439603805542\n",
      "time = 0.1025070826212565, epoch = 1, iter = 3620, loss = 5.5762073516845705\n",
      "time = 0.1079134980837504, epoch = 1, iter = 3630, loss = 5.606343364715576\n",
      "time = 0.08070311943689983, epoch = 1, iter = 3640, loss = 5.550629806518555\n",
      "time = 0.0763053297996521, epoch = 1, iter = 3650, loss = 5.612909317016602\n",
      "time = 0.11340286334355672, epoch = 1, iter = 3660, loss = 5.6693275451660154\n",
      "time = 0.13588115374247234, epoch = 1, iter = 3670, loss = 5.589623355865479\n",
      "time = 0.1381475845972697, epoch = 1, iter = 3680, loss = 5.603932905197143\n",
      "time = 0.11340666611989339, epoch = 1, iter = 3690, loss = 5.55754714012146\n",
      "time = 0.09481910069783529, epoch = 1, iter = 3700, loss = 5.603976821899414\n",
      "time = 0.08523706992467245, epoch = 1, iter = 3710, loss = 5.533936643600464\n",
      "time = 0.07574344873428344, epoch = 1, iter = 3720, loss = 5.544078063964844\n",
      "time = 0.07876519759496053, epoch = 1, iter = 3730, loss = 5.472179555892945\n",
      "time = 0.09217331409454346, epoch = 1, iter = 3740, loss = 5.61504807472229\n",
      "time = 0.08179168303807577, epoch = 1, iter = 3750, loss = 5.57714262008667\n",
      "time = 0.09831329584121704, epoch = 1, iter = 3760, loss = 5.625158977508545\n",
      "time = 0.10865238110224405, epoch = 1, iter = 3770, loss = 5.584631395339966\n",
      "time = 0.14494553009668987, epoch = 1, iter = 3780, loss = 5.572411775588989\n",
      "time = 0.13827383518218994, epoch = 1, iter = 3790, loss = 5.555043792724609\n",
      "time = 0.12425599892934164, epoch = 1, iter = 3800, loss = 5.628684043884277\n",
      "time = 0.07966670195261637, epoch = 1, iter = 3810, loss = 5.542713356018067\n",
      "time = 0.09008438189824422, epoch = 1, iter = 3820, loss = 5.437245082855225\n",
      "time = 0.07727798223495483, epoch = 1, iter = 3830, loss = 5.626977729797363\n",
      "time = 0.07745064894358317, epoch = 1, iter = 3840, loss = 5.630347490310669\n",
      "time = 0.08157895008722942, epoch = 1, iter = 3850, loss = 5.517253446578979\n",
      "time = 0.08852848609288534, epoch = 1, iter = 3860, loss = 5.603877115249634\n",
      "time = 0.09764273166656494, epoch = 1, iter = 3870, loss = 5.428012037277222\n",
      "time = 0.09896788199742636, epoch = 1, iter = 3880, loss = 5.593742990493775\n",
      "time = 0.11993303298950195, epoch = 1, iter = 3890, loss = 5.543291521072388\n",
      "time = 0.1266178806622823, epoch = 1, iter = 3900, loss = 5.419275283813477\n",
      "time = 0.14447571436564127, epoch = 1, iter = 3910, loss = 5.56131591796875\n",
      "time = 0.11788371404012045, epoch = 1, iter = 3920, loss = 5.571633005142212\n",
      "time = 0.09891827901204427, epoch = 1, iter = 3930, loss = 5.61439266204834\n",
      "time = 0.07721352974573771, epoch = 1, iter = 3940, loss = 5.591081857681274\n",
      "time = 0.0978047013282776, epoch = 1, iter = 3950, loss = 5.470348691940307\n",
      "time = 0.09053285121917724, epoch = 1, iter = 3960, loss = 5.5567864894866945\n",
      "time = 0.09398368199666342, epoch = 1, iter = 3970, loss = 5.518982601165772\n",
      "time = 0.08789005279541015, epoch = 1, iter = 3980, loss = 5.409684658050537\n",
      "time = 0.08230835199356079, epoch = 1, iter = 3990, loss = 5.477761697769165\n",
      "time = 0.08940778573354086, epoch = 1, iter = 4000, loss = 5.547434139251709\n",
      "time = 0.1490207354227702, epoch = 1, iter = 4010, loss = 5.404348564147949\n",
      "time = 0.1560876488685608, epoch = 1, iter = 4020, loss = 5.51059136390686\n",
      "time = 0.11309932072957357, epoch = 1, iter = 4030, loss = 5.506525468826294\n",
      "time = 0.10530083576838176, epoch = 1, iter = 4040, loss = 5.541067743301392\n",
      "time = 0.08033686478932699, epoch = 1, iter = 4050, loss = 5.494058084487915\n",
      "time = 0.08973728020985922, epoch = 1, iter = 4060, loss = 5.564924144744873\n",
      "time = 0.09147220055262248, epoch = 1, iter = 4070, loss = 5.479388189315796\n",
      "time = 0.11159131526947022, epoch = 1, iter = 4080, loss = 5.6463844776153564\n",
      "time = 0.09809578657150268, epoch = 1, iter = 4090, loss = 5.553496074676514\n",
      "time = 0.09683468341827392, epoch = 1, iter = 4100, loss = 5.45654525756836\n",
      "time = 0.10233449538548788, epoch = 1, iter = 4110, loss = 5.532306909561157\n",
      "time = 0.14280209938685098, epoch = 1, iter = 4120, loss = 5.566959047317505\n",
      "time = 0.1405246337254842, epoch = 1, iter = 4130, loss = 5.4963610649108885\n",
      "time = 0.1080377181371053, epoch = 1, iter = 4140, loss = 5.476466417312622\n",
      "time = 0.09682198762893676, epoch = 1, iter = 4150, loss = 5.409447717666626\n",
      "time = 0.09396671454111735, epoch = 1, iter = 4160, loss = 5.603194761276245\n",
      "time = 0.07479919592539469, epoch = 1, iter = 4170, loss = 5.5077465057373045\n",
      "time = 0.09861348072687785, epoch = 1, iter = 4180, loss = 5.555194520950318\n",
      "time = 0.11447542905807495, epoch = 1, iter = 4190, loss = 5.511187648773193\n",
      "time = 0.0812233289082845, epoch = 1, iter = 4200, loss = 5.461761522293091\n",
      "time = 0.10366700490315756, epoch = 1, iter = 4210, loss = 5.5038210391998295\n",
      "time = 0.11943978865941365, epoch = 1, iter = 4220, loss = 5.506453609466552\n",
      "time = 0.12073183457056681, epoch = 1, iter = 4230, loss = 5.514947557449341\n",
      "time = 0.14078696171442667, epoch = 1, iter = 4240, loss = 5.581380176544189\n",
      "time = 0.1299734155337016, epoch = 1, iter = 4250, loss = 5.5812126159667965\n",
      "time = 0.09090960423151652, epoch = 1, iter = 4260, loss = 5.539605236053466\n",
      "time = 0.0816312034924825, epoch = 1, iter = 4270, loss = 5.49378604888916\n",
      "time = 0.08641899824142456, epoch = 1, iter = 4280, loss = 5.476631164550781\n",
      "time = 0.09877949555714925, epoch = 1, iter = 4290, loss = 5.52187557220459\n",
      "time = 0.11194444894790649, epoch = 1, iter = 4300, loss = 5.482782936096191\n",
      "time = 0.08008926709493001, epoch = 1, iter = 4310, loss = 5.434239339828491\n",
      "time = 0.07929355303446452, epoch = 1, iter = 4320, loss = 5.46029806137085\n",
      "time = 0.0921344002087911, epoch = 1, iter = 4330, loss = 5.4544929504394535\n",
      "time = 0.11454163392384847, epoch = 1, iter = 4340, loss = 5.4381129264831545\n",
      "time = 0.1391085346539815, epoch = 1, iter = 4350, loss = 5.470412063598633\n",
      "time = 0.16435998280843098, epoch = 1, iter = 4360, loss = 5.439601802825928\n",
      "time = 0.10571694771448771, epoch = 1, iter = 4370, loss = 5.45178451538086\n",
      "time = 0.09524840116500854, epoch = 1, iter = 4380, loss = 5.4640320301055905\n",
      "time = 0.09129901727040608, epoch = 1, iter = 4390, loss = 5.47400131225586\n",
      "time = 0.10914812882741293, epoch = 1, iter = 4400, loss = 5.526347589492798\n",
      "time = 0.10933353503545125, epoch = 1, iter = 4410, loss = 5.5626379489898685\n",
      "time = 0.11006524960199991, epoch = 1, iter = 4420, loss = 5.404471302032471\n",
      "time = 0.10070804754892985, epoch = 1, iter = 4430, loss = 5.424112319946289\n",
      "time = 0.10465508302052816, epoch = 1, iter = 4440, loss = 5.488603687286377\n",
      "time = 0.9480725844701131, epoch = 1, iter = 4450, loss = 5.4389402866363525\n",
      "time = 0.16254811286926268, epoch = 1, iter = 4460, loss = 5.421503496170044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\u001b[38;5;241m3\u001b[39m, output_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs, output_interval)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(preds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), ys, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(3, output_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f48bc",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
