{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1dbb0c",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\" width=150px> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>ChatGPT and Friends</h1>\n",
    "<h1>Transformer</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f0117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import spacy\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e633f64",
   "metadata": {},
   "source": [
    "We start by printing out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86582a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.21.0\n",
      "\n",
      "Compiler    : Clang 15.0.0 (clang-1500.1.0.2.5)\n",
      "OS          : Darwin\n",
      "Release     : 23.3.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 1f87e80538ad172ebadf16b8ffe7f1e01f363ed6\n",
      "\n",
      "matplotlib: 3.8.2\n",
      "tqdm      : 4.66.1\n",
      "torchtext : 0.6.0\n",
      "spacy     : 3.7.2\n",
      "pandas    : 2.2.0\n",
      "torch     : 2.2.0\n",
      "numpy     : 1.26.4\n",
      "json      : 2.0.9\n",
      "watermark : 2.4.3\n",
      "networkx  : 3.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808da77",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a03c84",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c53c94",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3182a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3072a98",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6344441",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834981b",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae370e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant positional encoding matrix\n",
    "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
    "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
    "        seq_len = x.size()[1]\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af1d4d",
   "metadata": {},
   "source": [
    "## Model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de20b4",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c96df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given Query, Key, Value, calculate the final weighted value\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
    "    # Shape of v is (batch_size, seq_len, d_v)\n",
    "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Apply mask to scores\n",
    "    # <pad>\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
    "        \n",
    "    # Softmax along the last dimension\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d405d64",
   "metadata": {},
   "source": [
    "### Multi-Head Attention layer\n",
    "\n",
    "![](images/scaled_dot_product_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b945c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        # self attention linear layers\n",
    "        # Linear layers for q, k, v vectors generation in different heads\n",
    "        self.q_linear_layers = []\n",
    "        self.k_linear_layers = []\n",
    "        self.v_linear_layers = []\n",
    "        for i in range(n_heads):\n",
    "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        multi_head_attention_outputs = []\n",
    "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
    "                                                self.k_linear_layers,\n",
    "                                                self.v_linear_layers):\n",
    "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
    "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
    "            new_v = v_linear(v)  # size: (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Scaled Dot-Product attention\n",
    "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
    "            multi_head_attention_outputs.append(head_v)\n",
    "            \n",
    "        # Concat\n",
    "        #import pdb; pdb.set_trace()\n",
    "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
    "        \n",
    "        # Linear layer to recover to original shap\n",
    "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d2343",
   "metadata": {},
   "source": [
    "### Feed Forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17b121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211156a",
   "metadata": {},
   "source": [
    "### Layer Normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6aaed",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c99ce5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^{2} = \\frac{1}{m} \\sum^{m}_{i=1}(x_{i} - \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b826b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{Z}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma^{2}_{i} + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279adf27",
   "metadata": {},
   "source": [
    "#### Add two learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd71ba6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{Z}_i = \\alpha_i * \\hat{Z}_i + \\beta_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3ea7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len, d_model)\n",
    "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
    "        x_tilde = self.alpha*x_hat + self.beta\n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ff63c",
   "metadata": {},
   "source": [
    "## Encoder & Decoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42ab11",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "An encoder layer contains a multi-head attention layer and feed forward layer\n",
    "\n",
    "![](images/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2deef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        x = x + self.dropout_2(self.feed_forward(x))\n",
    "        x = self.norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfe012",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "An decoder layer contains two multi-head attention layers and one feed forward layer\n",
    "\n",
    "![](images/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d020ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.norm_3 = LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
    "        x = x + self.norm_1(x)\n",
    "        \n",
    "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
    "        x = x + self.norm_2(x)\n",
    "        \n",
    "        x = self.dropout_3(self.feed_forward(x))\n",
    "        x = x + self.norm_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d42ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_layer(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c68f1",
   "metadata": {},
   "source": [
    "## Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb91a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f158758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for decoder in self.decoder_layers:\n",
    "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec3c5f",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "![](images/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ba9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
    "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "        output = self.linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd6d4a",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ce235c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f0fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da11599",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(lower=True, tokenize=tokenizer)\n",
    "TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "832fb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = open('data/english.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2819e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_data = open('data/french.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a655df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bc1f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data, columns=['src', 'trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe269b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.\\n</td>\n",
       "      <td>Va !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Cours !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Courez !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!\\n</td>\n",
       "      <td>Au feu !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!\\n</td>\n",
       "      <td>À l'aide !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154878</th>\n",
       "      <td>\"Top-down economics never works,\" said Obama. ...</td>\n",
       "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154879</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154880</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154881</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154882</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154883 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0                                                   Go.\\n   \n",
       "1                                                  Run!\\n   \n",
       "2                                                  Run!\\n   \n",
       "3                                                 Fire!\\n   \n",
       "4                                                 Help!\\n   \n",
       "...                                                   ...   \n",
       "154878  \"Top-down economics never works,\" said Obama. ...   \n",
       "154879  A carbon footprint is the amount of carbon dio...   \n",
       "154880  Death is something that we're often discourage...   \n",
       "154881  Since there are usually multiple websites on a...   \n",
       "154882  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                      trg  \n",
       "0                                                  Va !\\n  \n",
       "1                                               Cours !\\n  \n",
       "2                                              Courez !\\n  \n",
       "3                                              Au feu !\\n  \n",
       "4                                            À l'aide !\\n  \n",
       "...                                                   ...  \n",
       "154878  « L'économie en partant du haut vers le bas, ç...  \n",
       "154879  Une empreinte carbone est la somme de pollutio...  \n",
       "154880  La mort est une chose qu'on nous décourage sou...  \n",
       "154881  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "154882  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "\n",
       "[154883 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12da949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/en_to_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8954aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f93dc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.TabularDataset('data/en_to_fr.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9f40e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f211360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14115"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8761114",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47b97f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28354"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bca6c6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x13de72290>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b6cfd",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "553a39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "N = 6\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd889365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af71817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/rlbcnmpd50bb89t5c4zqw6y80000gn/T/ipykernel_2389/1058947583.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b59128e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf2c7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train_set, batch_size=32, sort_key=lambda x: (len(x.src), len(x.trg)), shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dce007c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_input, trg_input):\n",
    "    # Source input mask\n",
    "    pad = SRC.vocab.stoi['<pad>']\n",
    "    src_mask = (src_input != pad).unsqueeze(1)\n",
    "    \n",
    "    # Target input mask\n",
    "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
    "    \n",
    "    seq_len = trg_input.size(1)\n",
    "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
    "    trg_mask = trg_mask & nopeak_mask\n",
    "    \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b80febc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, output_interval=100):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, batch in tqdm(enumerate(train_iter)):\n",
    "            \n",
    "            src_input = batch.src.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            trg = batch.trg.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create src & trg masks\n",
    "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
    "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i + 1) % output_interval == 0:\n",
    "                avg_loss = total_loss/output_interval\n",
    "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
    "                                                                           epoch + 1,\n",
    "                                                                           i + 1,\n",
    "                                                                           avg_loss))\n",
    "                total_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4f743df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6e671fe08c4ac0bedf3c68de419946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.7181087493896484, epoch = 1, iter = 10, loss = 9.744252777099609\n",
      "time = 0.5791713476181031, epoch = 1, iter = 20, loss = 9.11976146697998\n",
      "time = 0.49680381615956626, epoch = 1, iter = 30, loss = 8.52299451828003\n",
      "time = 0.5703819155693054, epoch = 1, iter = 40, loss = 7.964556455612183\n",
      "time = 0.7615645686785381, epoch = 1, iter = 50, loss = 7.460255908966064\n",
      "time = 0.5137118617693583, epoch = 1, iter = 60, loss = 6.950308465957642\n",
      "time = 0.5664933999379476, epoch = 1, iter = 70, loss = 6.59886417388916\n",
      "time = 0.6537155469258626, epoch = 1, iter = 80, loss = 6.3433913230896\n",
      "time = 0.6993738691012065, epoch = 1, iter = 90, loss = 6.246134471893311\n",
      "time = 0.5269575317700704, epoch = 1, iter = 100, loss = 6.102986574172974\n",
      "time = 0.5229846477508545, epoch = 1, iter = 110, loss = 6.151948738098144\n",
      "time = 0.552651564280192, epoch = 1, iter = 120, loss = 6.029599809646607\n",
      "time = 0.6153821349143982, epoch = 1, iter = 130, loss = 6.177250814437866\n",
      "time = 0.4864656647046407, epoch = 1, iter = 140, loss = 6.1213305473327635\n",
      "time = 0.5262103160222371, epoch = 1, iter = 150, loss = 6.042380857467651\n",
      "time = 0.5426400661468506, epoch = 1, iter = 160, loss = 6.115576171875\n",
      "time = 0.5580458839734396, epoch = 1, iter = 170, loss = 6.057501983642578\n",
      "time = 0.49036742051442467, epoch = 1, iter = 180, loss = 6.083053493499756\n",
      "time = 0.5363946318626404, epoch = 1, iter = 190, loss = 6.096358346939087\n",
      "time = 0.5505396525065104, epoch = 1, iter = 200, loss = 6.031374883651734\n",
      "time = 0.5330746491750081, epoch = 1, iter = 210, loss = 6.029800891876221\n",
      "time = 0.5992286205291748, epoch = 1, iter = 220, loss = 6.045639896392823\n",
      "time = 0.7530086676279704, epoch = 1, iter = 230, loss = 6.0293560981750485\n",
      "time = 0.530607267220815, epoch = 1, iter = 240, loss = 5.994535112380982\n",
      "time = 0.504287048180898, epoch = 1, iter = 250, loss = 6.064364337921143\n",
      "time = 0.5937281131744385, epoch = 1, iter = 260, loss = 5.992282104492188\n",
      "time = 0.7017496188481649, epoch = 1, iter = 270, loss = 5.985941076278687\n",
      "time = 0.7110724806785583, epoch = 1, iter = 280, loss = 6.011468935012817\n",
      "time = 0.8369355201721191, epoch = 1, iter = 290, loss = 6.042494010925293\n",
      "time = 0.7102990984916687, epoch = 1, iter = 300, loss = 5.961714267730713\n",
      "time = 0.7182733138402303, epoch = 1, iter = 310, loss = 6.013728761672974\n",
      "time = 0.8586271643638611, epoch = 1, iter = 320, loss = 5.970770406723022\n",
      "time = 0.8647628863652547, epoch = 1, iter = 330, loss = 6.027750158309937\n",
      "time = 0.6729875683784485, epoch = 1, iter = 340, loss = 5.917612791061401\n",
      "time = 0.6433848341306051, epoch = 1, iter = 350, loss = 6.022029781341553\n",
      "time = 0.790093195438385, epoch = 1, iter = 360, loss = 5.99307861328125\n",
      "time = 0.7218161980311076, epoch = 1, iter = 370, loss = 5.996657943725586\n",
      "time = 0.7423702041308086, epoch = 1, iter = 380, loss = 6.041255807876587\n",
      "time = 0.5827176690101623, epoch = 1, iter = 390, loss = 5.973006629943848\n",
      "time = 0.557388985157013, epoch = 1, iter = 400, loss = 6.05193133354187\n",
      "time = 0.5356895844141643, epoch = 1, iter = 410, loss = 6.013973188400269\n",
      "time = 0.5811753471692404, epoch = 1, iter = 420, loss = 5.958729791641235\n",
      "time = 0.6185019652048747, epoch = 1, iter = 430, loss = 5.959285116195678\n",
      "time = 0.5571644862492879, epoch = 1, iter = 440, loss = 5.996264314651489\n",
      "time = 0.4576747457186381, epoch = 1, iter = 450, loss = 5.932257270812988\n",
      "time = 0.5386867682139079, epoch = 1, iter = 460, loss = 5.963065719604492\n",
      "time = 0.5505727012952168, epoch = 1, iter = 470, loss = 6.01119909286499\n",
      "time = 0.5642072637875875, epoch = 1, iter = 480, loss = 6.055135869979859\n",
      "time = 0.5374378045399983, epoch = 1, iter = 490, loss = 5.965306997299194\n",
      "time = 0.473518431186676, epoch = 1, iter = 500, loss = 6.0110334873199465\n",
      "time = 0.45045340061187744, epoch = 1, iter = 510, loss = 6.037520885467529\n",
      "time = 0.43018471797307334, epoch = 1, iter = 520, loss = 6.022533178329468\n",
      "time = 0.4186702370643616, epoch = 1, iter = 530, loss = 6.0157064437866214\n",
      "time = 0.4836299538612366, epoch = 1, iter = 540, loss = 6.069160032272339\n",
      "time = 0.4550120194753011, epoch = 1, iter = 550, loss = 6.025240707397461\n",
      "time = 2.602955917517344, epoch = 1, iter = 560, loss = 6.009659051895142\n",
      "time = 179.5268939812978, epoch = 1, iter = 570, loss = 6.049966764450073\n",
      "time = 61.37440234422684, epoch = 1, iter = 580, loss = 6.027241373062134\n",
      "time = 1.4896392186482748, epoch = 1, iter = 590, loss = 6.072509241104126\n",
      "time = 59.620832713445026, epoch = 1, iter = 600, loss = 6.0564188957214355\n",
      "time = 23.37321379582087, epoch = 1, iter = 610, loss = 5.933726215362549\n",
      "time = 0.49716179768244423, epoch = 1, iter = 620, loss = 5.965111780166626\n",
      "time = 0.44898786544799807, epoch = 1, iter = 630, loss = 5.989701604843139\n",
      "time = 0.4403050502141317, epoch = 1, iter = 640, loss = 5.965627765655517\n",
      "time = 0.4761932849884033, epoch = 1, iter = 650, loss = 5.990492963790894\n",
      "time = 0.49420618613560996, epoch = 1, iter = 660, loss = 6.064835262298584\n",
      "time = 0.3986385385195414, epoch = 1, iter = 670, loss = 5.9876296043396\n",
      "time = 0.45503278573354083, epoch = 1, iter = 680, loss = 6.041199493408203\n",
      "time = 0.3770807464917501, epoch = 1, iter = 690, loss = 6.040417194366455\n",
      "time = 0.44124422073364256, epoch = 1, iter = 700, loss = 5.942631530761719\n",
      "time = 0.45883434613545737, epoch = 1, iter = 710, loss = 6.082478094100952\n",
      "time = 0.42941989898681643, epoch = 1, iter = 720, loss = 6.031559991836548\n",
      "time = 0.4476446350415548, epoch = 1, iter = 730, loss = 5.979284715652466\n",
      "time = 0.45458993514378865, epoch = 1, iter = 740, loss = 6.011397743225098\n",
      "time = 0.40011914968490603, epoch = 1, iter = 750, loss = 6.026323080062866\n",
      "time = 0.4212185859680176, epoch = 1, iter = 760, loss = 6.0257120609283445\n",
      "time = 0.3778523166974386, epoch = 1, iter = 770, loss = 5.999808502197266\n",
      "time = 0.4154809315999349, epoch = 1, iter = 780, loss = 6.0120782375335695\n",
      "time = 0.47400019963582357, epoch = 1, iter = 790, loss = 6.00641508102417\n",
      "time = 0.41226951678593954, epoch = 1, iter = 800, loss = 5.965056610107422\n",
      "time = 0.4046521504720052, epoch = 1, iter = 810, loss = 5.941239261627198\n",
      "time = 0.40146858294804894, epoch = 1, iter = 820, loss = 6.071232271194458\n",
      "time = 0.4163486957550049, epoch = 1, iter = 830, loss = 6.050233602523804\n",
      "time = 0.4412652651468913, epoch = 1, iter = 840, loss = 6.007892990112305\n",
      "time = 0.41599666674931846, epoch = 1, iter = 850, loss = 6.056145095825196\n",
      "time = 0.39150032997131345, epoch = 1, iter = 860, loss = 6.021520376205444\n",
      "time = 0.4420431216557821, epoch = 1, iter = 870, loss = 6.042748546600341\n",
      "time = 0.4474307815233866, epoch = 1, iter = 880, loss = 5.978354597091675\n",
      "time = 0.45102156400680543, epoch = 1, iter = 890, loss = 6.026226282119751\n",
      "time = 0.48377211888631183, epoch = 1, iter = 900, loss = 6.024500226974487\n",
      "time = 0.4176259160041809, epoch = 1, iter = 910, loss = 6.0372038841247555\n",
      "time = 0.4505643486976624, epoch = 1, iter = 920, loss = 5.970793437957764\n",
      "time = 0.4479111870129903, epoch = 1, iter = 930, loss = 6.038093042373657\n",
      "time = 0.3987854838371277, epoch = 1, iter = 940, loss = 6.037785482406616\n",
      "time = 0.459019136428833, epoch = 1, iter = 950, loss = 5.981096267700195\n",
      "time = 0.4582509477933248, epoch = 1, iter = 960, loss = 6.014000606536865\n",
      "time = 0.4089863816897074, epoch = 1, iter = 970, loss = 5.971947908401489\n",
      "time = 0.4657362143198649, epoch = 1, iter = 980, loss = 6.064471435546875\n",
      "time = 0.48858798344930016, epoch = 1, iter = 990, loss = 5.977895355224609\n",
      "time = 0.43743216594060264, epoch = 1, iter = 1000, loss = 5.9846419334411625\n",
      "time = 0.39302136103312174, epoch = 1, iter = 1010, loss = 6.034248304367066\n",
      "time = 0.466518235206604, epoch = 1, iter = 1020, loss = 5.935423374176025\n",
      "time = 0.4826577027638753, epoch = 1, iter = 1030, loss = 5.975125122070312\n",
      "time = 0.42867613236109414, epoch = 1, iter = 1040, loss = 6.001387357711792\n",
      "time = 0.4342457175254822, epoch = 1, iter = 1050, loss = 6.025126552581787\n",
      "time = 0.46645257075627644, epoch = 1, iter = 1060, loss = 5.959999513626099\n",
      "time = 0.4204356034596761, epoch = 1, iter = 1070, loss = 6.027868556976318\n",
      "time = 0.4294384797414144, epoch = 1, iter = 1080, loss = 6.0130431175231935\n",
      "time = 0.4533921480178833, epoch = 1, iter = 1090, loss = 6.0287117004394535\n",
      "time = 0.433306352297465, epoch = 1, iter = 1100, loss = 5.999097108840942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.4171146869659424, epoch = 1, iter = 1110, loss = 6.0333961009979244\n",
      "time = 0.4102659344673157, epoch = 1, iter = 1120, loss = 5.968945741653442\n",
      "time = 0.4905858357747396, epoch = 1, iter = 1130, loss = 5.959720468521118\n",
      "time = 0.5472680846850078, epoch = 1, iter = 1140, loss = 5.998389482498169\n",
      "time = 0.56587895154953, epoch = 1, iter = 1150, loss = 6.0517371654510494\n",
      "time = 0.4736044486363729, epoch = 1, iter = 1160, loss = 5.965520238876342\n",
      "time = 0.3972519318262736, epoch = 1, iter = 1170, loss = 6.0142923355102536\n",
      "time = 0.4450522502263387, epoch = 1, iter = 1180, loss = 6.068280220031738\n",
      "time = 0.5478315989176432, epoch = 1, iter = 1190, loss = 5.890934181213379\n",
      "time = 0.5642948150634766, epoch = 1, iter = 1200, loss = 6.063024091720581\n",
      "time = 0.5525031010309855, epoch = 1, iter = 1210, loss = 5.967985343933106\n",
      "time = 0.7016456166903178, epoch = 1, iter = 1220, loss = 6.040849018096924\n",
      "time = 0.5861648003260295, epoch = 1, iter = 1230, loss = 5.929404640197754\n",
      "time = 0.5468541701634725, epoch = 1, iter = 1240, loss = 5.941571998596191\n",
      "time = 0.5408947865168253, epoch = 1, iter = 1250, loss = 6.006755113601685\n",
      "time = 0.5481298128763835, epoch = 1, iter = 1260, loss = 5.9947185039520265\n",
      "time = 0.5552887678146362, epoch = 1, iter = 1270, loss = 6.00619330406189\n",
      "time = 0.6881549477577209, epoch = 1, iter = 1280, loss = 6.0194329738616945\n",
      "time = 0.6143230636914571, epoch = 1, iter = 1290, loss = 5.992401599884033\n",
      "time = 0.7099886496861776, epoch = 1, iter = 1300, loss = 6.001006603240967\n",
      "time = 0.8033186793327332, epoch = 1, iter = 1310, loss = 5.9641927719116214\n",
      "time = 0.5482815146446228, epoch = 1, iter = 1320, loss = 5.89412260055542\n",
      "time = 0.45479251543680826, epoch = 1, iter = 1330, loss = 6.025849485397339\n",
      "time = 0.4397722999254862, epoch = 1, iter = 1340, loss = 6.026773071289062\n",
      "time = 0.4522813320159912, epoch = 1, iter = 1350, loss = 6.009773445129395\n",
      "time = 0.4315215826034546, epoch = 1, iter = 1360, loss = 5.915852069854736\n",
      "time = 0.4653311848640442, epoch = 1, iter = 1370, loss = 5.919200849533081\n",
      "time = 0.4182958483695984, epoch = 1, iter = 1380, loss = 5.966216421127319\n",
      "time = 0.4372382998466492, epoch = 1, iter = 1390, loss = 5.899897146224975\n",
      "time = 0.43938321272532144, epoch = 1, iter = 1400, loss = 5.908613586425782\n",
      "time = 0.4696837067604065, epoch = 1, iter = 1410, loss = 5.861124563217163\n",
      "time = 0.49280678033828734, epoch = 1, iter = 1420, loss = 5.975657510757446\n",
      "time = 0.45980941454569496, epoch = 1, iter = 1430, loss = 5.905686378479004\n",
      "time = 0.45778618653615316, epoch = 1, iter = 1440, loss = 5.887841606140137\n",
      "time = 0.5107377688090007, epoch = 1, iter = 1450, loss = 6.029988479614258\n",
      "time = 0.5170640031496684, epoch = 1, iter = 1460, loss = 5.8676833152771\n",
      "time = 0.5034663836161296, epoch = 1, iter = 1470, loss = 5.909590387344361\n",
      "time = 0.43182058334350587, epoch = 1, iter = 1480, loss = 5.880030012130737\n",
      "time = 0.47161525090535483, epoch = 1, iter = 1490, loss = 5.8626380443573\n",
      "time = 0.4299906810124715, epoch = 1, iter = 1500, loss = 5.780824899673462\n",
      "time = 0.4460056821505229, epoch = 1, iter = 1510, loss = 5.83099102973938\n",
      "time = 0.5304257829984029, epoch = 1, iter = 1520, loss = 5.944102716445923\n",
      "time = 0.43853623469670616, epoch = 1, iter = 1530, loss = 5.899747896194458\n",
      "time = 0.4153313835461934, epoch = 1, iter = 1540, loss = 5.913488435745239\n",
      "time = 0.41844625075658165, epoch = 1, iter = 1550, loss = 5.917034482955932\n",
      "time = 0.44545501867930093, epoch = 1, iter = 1560, loss = 5.899103307723999\n",
      "time = 0.43327583074569703, epoch = 1, iter = 1570, loss = 5.822604751586914\n",
      "time = 0.49618935187657676, epoch = 1, iter = 1580, loss = 5.857733535766601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs, output_interval)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# create src & trg masks\u001b[39;00m\n\u001b[1;32m     17\u001b[0m src_mask, trg_mask \u001b[38;5;241m=\u001b[39m create_mask(src_input, trg_input)\n\u001b[0;32m---> 18\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(preds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), ys, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, trg, src_mask, trg_mask):\n\u001b[1;32m      9\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, src_mask)\n\u001b[0;32m---> 10\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(decoder_output)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, trg, encoder_output, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(x)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, encoder_output, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, trg_mask):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m v_linear(v)  \u001b[38;5;66;03m# size: (batch_size, seq_len, d_v)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Scaled Dot-Product attention\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     head_v \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_len, d_v)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     multi_head_attention_outputs\u001b[38;5;241m.\u001b[39mappend(head_v)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Concat\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#import pdb; pdb.set_trace()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask, dropout)\u001b[0m\n\u001b[1;32m     10\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Softmax along the last dimension\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m dropout(attention_weights)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/functional.py:1828\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \n\u001b[1;32m   1831\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \n\u001b[1;32m   1852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(3, output_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f48bc",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
